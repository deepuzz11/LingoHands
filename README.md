# ğŸ¤Ÿ LingoHands, an AI-Powered ASL Detection : Translating Gestures, Connecting Worlds ğŸŒğŸ¤

## ğŸŒ Bridging the Communication Gap

LingoHands is an AI-powered real-time American Sign Language (ASL) detection system designed to enhance accessibility for the hearing-impaired. By leveraging deep learning and computer vision, it accurately recognizes ASL signs from images and video streams, fostering inclusivity in a communication-driven world.

---
## âœ¨ Features
âœ… **ASL Alphabet Recognition** â€“ Detects all 26 ASL letters plus special signs (space, delete, nothing).  
âœ… **Deep Learning Model** â€“ Powered by a CNN for precise sign language interpretation.  
âœ… **Real-Time Prediction** â€“ Recognizes ASL gestures instantly from a webcam feed.  
âœ… **Preprocessed Dataset** â€“ Utilizes a curated ASL dataset for robust training and validation.  
âœ… **User-Friendly Interface** â€“ Easily deployable for real-world applications.  

---
## ğŸ“‚ Dataset Details
LingoHands is trained on the **Kaggle ASL Alphabet dataset**, which includes:
- **26 Classes**: Letters A-Z plus additional gestures.
- **Optimized Image Processing**: Images resized to 64x64 pixels for efficient model training.
- **Comprehensive Test Set**: Ensures accurate model evaluation.

The dataset was preprocessed using OpenCV and NumPy, including normalization and resizing, to ensure optimal performance during model training.

---
## ğŸ§  Model Architecture
LingoHands employs a **Convolutional Neural Network (CNN)** for image classification, comprising:
1ï¸âƒ£ **Convolutional Layers** â€“ Extracts essential features from input images.  
2ï¸âƒ£ **MaxPooling Layers** â€“ Reduces spatial dimensions and minimizes overfitting.  
3ï¸âƒ£ **Fully Connected Layers** â€“ Maps extracted features to ASL sign labels.  
4ï¸âƒ£ **Softmax Activation** â€“ Predicts the most probable ASL sign.  

The model was trained using TensorFlow and Keras, with categorical cross-entropy as the loss function and Adam optimizer for efficient learning.

---
## ğŸ¯ Training & Results
ğŸš€ **Training was conducted on Google Colab**, using GPU acceleration for faster computation. The training process involved:
- **Batch size of 32** and **10 epochs** for efficient learning.
- **80-20 train-validation split** to ensure generalization.
- **Accuracy of over 90%** achieved on validation data.

### ğŸ“ˆ Performance metrics include:
- Training and validation accuracy/loss trends.
- Real-time testing with new ASL sign images.

### ğŸ“Š Model Performance Visualization
Below is an example of accuracy and loss curves:

```python
import matplotlib.pyplot as plt

# Plot accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Model Accuracy')
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Model Loss')
plt.show()
```
![image](https://github.com/user-attachments/assets/b7a35b04-f042-4d75-a671-0fe38b6d6ba8)

---
## ğŸŒ Real-World Applications
LingoHands is more than just an AI model; itâ€™s a step toward accessibility and inclusion. Possible applications include:
- ğŸ”¹ **ASL Translation Apps** â€“ Enables real-time communication between deaf and hearing individuals.
- ğŸ”¹ **Educational Tools** â€“ Assists ASL learners with interactive practice.
- ğŸ”¹ **Smart Assistants** â€“ Recognizes ASL gestures for seamless smart home integration.
- ğŸ”¹ **Healthcare & Emergency Services** â€“ Helps patients communicate with medical professionals.

---
## ğŸ¬ Demo
To test the model with a sample ASL image:
```python
from tensorflow.keras.models import load_model
import cv2
import numpy as np

# Load the trained model
model = load_model("LingoHands_CNN_Model.h5")

def predict_asl_image(img_path):
    img = cv2.imread(img_path)
    img = cv2.resize(img, (64, 64))
    img = img / 255.0  # Normalize
    img = np.expand_dims(img, axis=0)  # Add batch dimension

    prediction = model.predict(img)
    predicted_label = np.argmax(prediction)
    class_name = classes[predicted_label]

    print(f"Predicted Sign: {class_name}")
    return class_name

# Example usage
sample_img_path = "path/to/sample_image.jpg"
predict_asl_image(sample_img_path)
```
![image](https://github.com/user-attachments/assets/7a4cfe9a-d320-4312-bda0-5baa7c89017c)

---
## ğŸ”® Future Enhancements
ğŸš€ Support for **dynamic ASL gestures** beyond alphabets.  
ğŸ“² Deployment as a **web & mobile app**.  
âš¡ Optimization for **faster real-time processing**.  
ğŸ¤– Integration with **voice assistants** for seamless communication.  

---
